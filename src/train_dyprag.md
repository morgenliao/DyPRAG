train_dyprag.py

# 1. 代码说明

## 1.1. 主要功能

该代码的主要用途是进行 DyPRAG（Dynamic Projection-based Parameter Regression）训练的第二阶段，用于训练一个投影器（projector），该投影器能够将特定于任务的参数（例如 LoRA 适配器的权重）投射到大型语言模型中。这个阶段是为了提高模型在特定任务上的性能。

## 1.2. 架构说明

代码的整体架构包括以下几个关键部分：

- 数据准备：从多个数据集加载和预处理数据，创建一个用于训练的 PyTorch 数据集。
- 投影器模型：定义了一个投影器类，它将学习如何将输入嵌入映射到 LoRA 适配器的权重。
- 训练循环：设置训练过程，包括损失函数的计算、反向传播和参数更新。
- 参数注入和移除：在训练过程中动态地注入和移除 LoRA 适配器的权重。

## 1.3. 关键组件

以下列出主要的类和函数：

- `create_lora_passage_dataset`：创建一个 PyTorch 数据集类，用于处理 LoRA 适配器和对应文本的配对数据。
- `prepare_training_data_multi_datasets`：准备来自多个数据集的训练数据。
- `TrainingDataCollator`：自定义数据收集器类，用于处理和打包训练数据。
- `main`：程序的入口点，它定义了训练过程，初始化模型和投影器，以及执行训练循环。
- `Projector`（未在提供的代码段中定义）：投影器类，用于将输入嵌入映射到 LoRA 适配器的权重。
- `delta_inject` 和 `delta_remove`：用于在训练过程中注入和移除 LoRA 适配器的权重的辅助函数。

此外，代码还使用了`peft`库来处理 LoRA 适配器，以及 PyTorch 和其它相关的机器学习库来执行模型训练。

# 2. 类分析

以下是代码中定义的类的详细分析：

TrainingDataCollator

以下是对给定类的分析：

## 2.1. 功能描述

该类`TrainingDataCollator`继承自`DefaultDataCollator`，主要用于准备训练数据。它接受一批示例数据，并将每个示例转换为模型可以接受的格式。这个类特别之处在于它还提取了每个示例的最后一个隐藏状态作为输入嵌入，并保存了适配器路径信息。

## 2.2. 参数说明

- `tokenizer`: 分词器，用于将文本转换为模型可以理解的 tokens。
- `device`: 指定模型和输入张量应该运行的硬件设备（如 CPU 或 GPU）。
- `model`: 训练中所使用的模型，用于提取隐藏状态。

## 2.3. 返回值

返回一个字典，包含以下键值对：

- `input_embeds`: 包含每个示例的最后一个隐藏状态的列表。
- `adapter_paths`: 每个示例对应的适配器路径的列表。
- `model_inputs`: 包含模型输入所需的所有张量的列表，如`input_ids`、`labels`和`attention_mask`。

## 2.4. 实现逻辑

- 构造函数`__init__`初始化了类成员变量`tokenizer`、`device`和`model`。
- `__call__`方法被重写，允许对象像函数一样被调用。它接受一个包含多个示例的列表，每个示例是一个字典，包含诸如`passage_tokens`、`input_ids`、`labels`和`attention_mask`的信息。
- 在`__call__`方法中，对每个示例执行以下步骤：
  - 使用`tokenizer`对文本进行分词。
  - 使用`model`和`torch.no_grad()`（以避免计算梯度）来获取最后一个隐藏状态。
  - 将最后一个隐藏状态添加到`input_embeds`列表。
  - 保存`adapter_path`。
  - 将`input_ids`、`labels`和`attention_mask`转换为张量，并移动到`device`上，然后添加到`model_inputs`列表。
- 最后，返回包含所有必要信息的字典。

这个类的目的是在训练循环中为模型提供预处理后的数据，特别是针对那些需要额外适配器路径信息的模型。

# 3. 函数分析

以下是代码中定义的函数的详细分析：

create_lora_passage_dataset

## 3.1. 功能描述

该函数的主要功能是从给定的 LoRa-passage 对中创建一个 PyTorch 数据集。这种类型的数据集通常用于机器学习任务，特别是在自然语言处理（NLP）中，其中 LoRa（Low-Rank Adaptation）可能指的是某种特定格式的数据适配。

## 3.2. 参数说明

- `lora_passage_pairs`: 这是一个列表或者其他可迭代的数据结构，其中包含了一系列的 LoRa-passage 对。每个对可能由两个元素组成，例如一个 LoRa 数据和它的对应 passage。

## 3.3. 返回值

- 返回一个`LoRAPassageDataset`类的实例，这是一个继承自`torch.utils.data.Dataset`的自定义数据集类。这个数据集可以在 PyTorch 中用于数据的加载和迭代。

## 3.4. 实现逻辑

1. 导入`torch.utils.data.Dataset`类，这是 PyTorch 中用于创建自定义数据集的基类。
2. 定义一个内部类`LoRAPassageDataset`，它继承自`Dataset`。
   - `__init__`方法接收一个名为`pairs`的参数，它被设置为类的属性。这里，`pairs`就是传递给函数的`lora_passage_pairs`。
   - `__len__`方法返回数据集中 LoRa-passage 对的数量，即`pairs`的长度。
   - `__getitem__`方法用于获取索引`idx`对应的 LoRa-passage 对。它简单地从`pairs`中检索并返回相应的元素。
3. 在函数的最后，它实例化`LoRAPassageDataset`类，将`lora_passage_pairs`作为参数传递，并返回这个数据集实例。

注意：该函数假设`lora_passage_pairs`已经是适当格式化且可以直接使用的。它没有执行任何数据预处理或清洗，这些步骤通常在数据集创建之前完成。

prepare_training_data_multi_datasets

## 3.5. 功能描述

该函数`prepare_training_data_multi_datasets`的主要功能是从多个数据集准备训练数据。它将处理不同的数据集，根据数据集的名称可能会有不同的处理逻辑，例如是否使用上下文提示（COT）或者不同的采样率。函数将数据集中的样本与相应的标签、注意力掩码等组合，并将它们存储在一个列表中，最后返回这个列表。

## 3.6. 参数说明

- `args`: 一个包含多个参数的变量，可能是一个命名空间或字典，包含了训练数据的配置信息，如数据集名称、采样率、模型名称、学习率等。
- `tokenizer`: 分词器对象，用于将文本转换为模型可接受的输入格式。
- `datasets`: 一个包含数据集名称的列表，用于指定要处理的数据集。

## 3.7. 返回值

返回一个列表`all_training_samples`，其中包含字典形式的训练样本。每个字典包含以下键：

- `adapter_path`: 适配器路径，用于存储特定的模型参数。
- `passage`: 原始文本段落。
- `passage_tokens`: 段落的分词结果。
- `input_ids`: 输入序列的 ID 表示。
- `labels`: 标签序列的 ID 表示。
- `attention_mask`: 注意力掩码，用于指示哪些位置是有效输入。
- `file_name`: 数据文件名。
- `data_id`: 数据样本 ID。
- `passage_id`: 段落 ID。
- `dataset`: 数据集名称。

返回的列表将包含来自所有指定数据集的准备好的训练样本。

## 3.8. 实现逻辑

1. 初始化一个空列表`all_training_samples`，用于存储所有训练样本。
2. 定义忽略的 ID（`ignored_id`），最大序列长度（`max_length`），以及填充 ID（`pad_token_id`）。
3. 遍历传入的`datasets`列表，针对每个数据集：
   - 设置特定的配置参数，如是否使用上下文提示（COT）。
   - 加载数据集的样本数据。
   - 对于每个数据文件，随机采样一定比例的数据（由`args.sample_rate`控制）。
   - 遍历样本，获取每个样本的增强数据，并生成训练数据所需的各项信息（如`input_ids`、`labels`、`attention_mask`等）。
   - 检查适配器路径是否存在，如果存在，将数据样本添加到`all_training_samples`列表中。
4. 打印准备好的训练样本总数。
5. 返回包含所有训练样本的列表。

main

下面是根据您提供的代码，按照指定格式进行的函数分析：

## 3.9. 功能描述

该函数的主要功能是训练一个名为 DyPRAG 的投影模型，该模型用于微调大型语言模型中的特定部分，即 LoRa 适配器。这个过程包括准备训练数据、初始化模型和投影器、执行训练循环、计算损失（包括语言建模损失、KL 散度损失和均方误差损失），并在每个训练阶段保存投影器模型。

## 3.10. 参数说明

- `args`：一个包含多个参数的命名空间或字典，这些参数用于配置模型、训练过程和设备选择。主要使用的参数包括：
  - `datasets`：包含训练数据的数据集。
  - `model_name`：指定基础模型的名称。
  - `max_new_tokens`：指定生成配置中的最大新令牌数。
  - `lora_rank`：指定 LoRa 适配器的秩。
  - `lora_alpha`：指定 LoRa 适配器的超参数。
  - `projector_p`：指定投影器的隐藏层数量。
  - `dyprag_train_epochs`：指定 DyPRAG 训练的周期数。
  - `dyprag_learning_rate`：指定投影器训练的学习率。
  - `sample_rate`：指定训练样本的采样率。

## 3.11. 返回值

该函数没有明确的返回值。它通过打印信息提供训练进度，并在训练完成后保存训练好的投影器模型。

## 3.12. 实现逻辑

1. 根据设备可用性，选择模型和投影器使用的设备（`cuda:0`或`cuda:1`，如果不可用则使用 CPU）。
2. 获取基础模型、分词器和生成配置。
3. 初始化 LoRa 配置，并获取带有 LoRa 适配器的模型。
4. 准备多数据集的训练样本。
5. 创建 LoRa passage 数据集，并初始化训练数据加载器。
6. 初始化投影器模型，并将其设置为训练模式。
7. 设置模型为评估模式，并初始化优化器。
8. 训练循环开始：
   - 对于每个训练周期，遍历数据加载器中的批次。
   - 在每个步骤中，清空梯度，加载 LoRa 适配器状态，计算原始 LM 损失，并重置 LoRa 参数。
   - 使用投影器生成输出，并将这些输出注入到模型中。
   - 计算注入后的 LM 损失、KL 损失和 MSE 损失。
   - 计算总损失，并执行反向传播和优化器步进。
   - 移除模型中的变化，并清理缓存。
   - 打印损失信息。
   - 在每个周期结束时，保存投影器模型的状态。
9. 打印训练完成信息。

请注意，这个分析是基于代码片段提供的，实际函数可能还包括其他未展示的逻辑和辅助函数。
